{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# enter the foldername in your Drive where you have saved the unzipped\n",
    "# 'cs231n' folder containing the '.py', 'classifiers' and 'datasets'\n",
    "# folders.\n",
    "# e.g. 'cs231n/assignments/assignment1/cs231n/'\n",
    "FOLDERNAME = None\n",
    "\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "%cd drive/My\\ Drive\n",
    "%cp -r $FOLDERNAME ../../\n",
    "%cd ../../\n",
    "%cd cs231n/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "5.293686120872577\n",
      "(10,)\n",
      "5.865260025887528\n",
      "(10,)\n",
      "6.472848310286401\n",
      "(10,)\n",
      "6.8327667502769405\n",
      "(10,)\n",
      "7.359079032740174\n",
      "(10,)\n",
      "6.605480912083242\n",
      "(10,)\n",
      "8.210873891945639\n",
      "(10,)\n",
      "7.879156205096716\n",
      "(10,)\n",
      "6.581273730378794\n",
      "(10,)\n",
      "4.72152852765093\n",
      "(10,)\n",
      "7.962256678392085\n",
      "(10,)\n",
      "6.3466475634663935\n",
      "(10,)\n",
      "5.363365433385258\n",
      "(10,)\n",
      "7.241702442335579\n",
      "(10,)\n",
      "4.837133958884887\n",
      "(10,)\n",
      "5.930664502431071\n",
      "(10,)\n",
      "5.9648611255034485\n",
      "(10,)\n",
      "6.74982728562615\n",
      "(10,)\n",
      "6.760418137226443\n",
      "(10,)\n",
      "8.412208243269632\n",
      "(10,)\n",
      "5.597933773887435\n",
      "(10,)\n",
      "7.440306176083606\n",
      "(10,)\n",
      "4.46360789592932\n",
      "(10,)\n",
      "4.102326652118493\n",
      "(10,)\n",
      "6.438427757387558\n",
      "(10,)\n",
      "5.859573851083192\n",
      "(10,)\n",
      "4.419766488282313\n",
      "(10,)\n",
      "7.503252570729031\n",
      "(10,)\n",
      "3.4260480136642877\n",
      "(10,)\n",
      "4.375145845439286\n",
      "(10,)\n",
      "5.717639202320616\n",
      "(10,)\n",
      "5.263807267944037\n",
      "(10,)\n",
      "5.606874319145851\n",
      "(10,)\n",
      "7.496769656335791\n",
      "(10,)\n",
      "5.709942973126552\n",
      "(10,)\n",
      "7.545697754239707\n",
      "(10,)\n",
      "6.790366058726769\n",
      "(10,)\n",
      "7.322008494580743\n",
      "(10,)\n",
      "5.58976910840249\n",
      "(10,)\n",
      "6.841449533957255\n",
      "(10,)\n",
      "5.926902948892305\n",
      "(10,)\n",
      "6.01178687249137\n",
      "(10,)\n",
      "6.8854672575370355\n",
      "(10,)\n",
      "5.812838335811306\n",
      "(10,)\n",
      "5.831236654307335\n",
      "(10,)\n",
      "4.887535017605254\n",
      "(10,)\n",
      "5.337841991271875\n",
      "(10,)\n",
      "4.879907250882561\n",
      "(10,)\n",
      "7.332551890058207\n",
      "(10,)\n",
      "4.579102268181196\n",
      "(10,)\n",
      "5.285486243132535\n",
      "(10,)\n",
      "7.433616713238493\n",
      "(10,)\n",
      "6.3700637800211215\n",
      "(10,)\n",
      "6.656114270211726\n",
      "(10,)\n",
      "5.402677056923199\n",
      "(10,)\n",
      "7.06197020904137\n",
      "(10,)\n",
      "7.662612122804334\n",
      "(10,)\n",
      "7.482219070444225\n",
      "(10,)\n",
      "6.632537314286659\n",
      "(10,)\n",
      "7.069094830281479\n",
      "(10,)\n",
      "6.688400918353679\n",
      "(10,)\n",
      "7.098314756873383\n",
      "(10,)\n",
      "8.172435949751263\n",
      "(10,)\n",
      "6.310750128175527\n",
      "(10,)\n",
      "5.923120733391317\n",
      "(10,)\n",
      "6.0227348926073105\n",
      "(10,)\n",
      "7.5638271539424045\n",
      "(10,)\n",
      "7.047634208607653\n",
      "(10,)\n",
      "5.663573740063863\n",
      "(10,)\n",
      "7.248574332400722\n",
      "(10,)\n",
      "7.083715913674752\n",
      "(10,)\n",
      "5.648667305037474\n",
      "(10,)\n",
      "5.918634397348905\n",
      "(10,)\n",
      "3.3632971346812677\n",
      "(10,)\n",
      "7.446934669649937\n",
      "(10,)\n",
      "8.234202814938111\n",
      "(10,)\n",
      "7.600163314928189\n",
      "(10,)\n",
      "7.1322832189313\n",
      "(10,)\n",
      "5.038027397809335\n",
      "(10,)\n",
      "6.0662810424838876\n",
      "(10,)\n",
      "7.899271785646058\n",
      "(10,)\n",
      "3.5125105596031005\n",
      "(10,)\n",
      "6.7086151856297604\n",
      "(10,)\n",
      "5.0270552189801085\n",
      "(10,)\n",
      "6.730282943194876\n",
      "(10,)\n",
      "5.064492190718485\n",
      "(10,)\n",
      "5.82213240707084\n",
      "(10,)\n",
      "6.731221482151714\n",
      "(10,)\n",
      "5.567287776022356\n",
      "(10,)\n",
      "6.781512455726363\n",
      "(10,)\n",
      "7.932678686944866\n",
      "(10,)\n",
      "8.670037220186838\n",
      "(10,)\n",
      "5.444847037311431\n",
      "(10,)\n",
      "6.8609922356334945\n",
      "(10,)\n",
      "6.6505824556958135\n",
      "(10,)\n",
      "5.039077261922727\n",
      "(10,)\n",
      "6.27776858775102\n",
      "(10,)\n",
      "6.226413259488188\n",
      "(10,)\n",
      "6.397626866778885\n",
      "(10,)\n",
      "7.227710745413493\n",
      "(10,)\n",
      "7.859488545139349\n",
      "(10,)\n",
      "6.88473744263052\n",
      "(10,)\n",
      "5.550216630460888\n",
      "(10,)\n",
      "5.717068751714731\n",
      "(10,)\n",
      "6.344502472597506\n",
      "(10,)\n",
      "5.8762232295325765\n",
      "(10,)\n",
      "7.352992841230722\n",
      "(10,)\n",
      "5.790446529226486\n",
      "(10,)\n",
      "4.365180978069925\n",
      "(10,)\n",
      "5.865398303842747\n",
      "(10,)\n",
      "6.519716946729124\n",
      "(10,)\n",
      "5.241202751808856\n",
      "(10,)\n",
      "4.044454798817117\n",
      "(10,)\n",
      "3.3931350675842067\n",
      "(10,)\n",
      "5.7755481929486745\n",
      "(10,)\n",
      "7.412192484025959\n",
      "(10,)\n",
      "6.418411486839107\n",
      "(10,)\n",
      "5.263013663646966\n",
      "(10,)\n",
      "6.571432596115353\n",
      "(10,)\n",
      "7.789367275769075\n",
      "(10,)\n",
      "4.222323011600212\n",
      "(10,)\n",
      "4.315431083013829\n",
      "(10,)\n",
      "5.487879599522634\n",
      "(10,)\n",
      "3.7814158749744546\n",
      "(10,)\n",
      "4.015241496689571\n",
      "(10,)\n",
      "6.322307758498609\n",
      "(10,)\n",
      "5.638850584218195\n",
      "(10,)\n",
      "7.067897672228253\n",
      "(10,)\n",
      "7.382240901074812\n",
      "(10,)\n",
      "7.587569317469156\n",
      "(10,)\n",
      "4.956123178536154\n",
      "(10,)\n",
      "6.251594330265275\n",
      "(10,)\n",
      "8.356639485209445\n",
      "(10,)\n",
      "5.656720589083257\n",
      "(10,)\n",
      "7.658584405791517\n",
      "(10,)\n",
      "6.479599809127788\n",
      "(10,)\n",
      "6.305563890964161\n",
      "(10,)\n",
      "4.60109321777615\n",
      "(10,)\n",
      "6.535416410277909\n",
      "(10,)\n",
      "6.818450266411431\n",
      "(10,)\n",
      "8.511027222041449\n",
      "(10,)\n",
      "5.458955867128239\n",
      "(10,)\n",
      "6.735375552959688\n",
      "(10,)\n",
      "6.714273791775268\n",
      "(10,)\n",
      "4.0344221539820415\n",
      "(10,)\n",
      "5.728383949550855\n",
      "(10,)\n",
      "3.8619490234627376\n",
      "(10,)\n",
      "5.275106422900016\n",
      "(10,)\n",
      "5.087162956470002\n",
      "(10,)\n",
      "7.6717123134168315\n",
      "(10,)\n",
      "6.5440534434780755\n",
      "(10,)\n",
      "5.848364729592887\n",
      "(10,)\n",
      "5.676517337467673\n",
      "(10,)\n",
      "6.64515842037022\n",
      "(10,)\n",
      "7.41570423846553\n",
      "(10,)\n",
      "7.493163711560131\n",
      "(10,)\n",
      "7.732579052588234\n",
      "(10,)\n",
      "5.10433115301918\n",
      "(10,)\n",
      "4.676665489087176\n",
      "(10,)\n",
      "4.130927872542228\n",
      "(10,)\n",
      "6.271006242191332\n",
      "(10,)\n",
      "5.6106833020989\n",
      "(10,)\n",
      "7.595337099247539\n",
      "(10,)\n",
      "5.904238706450518\n",
      "(10,)\n",
      "5.156826399455383\n",
      "(10,)\n",
      "7.2873989884449095\n",
      "(10,)\n",
      "6.427655416150773\n",
      "(10,)\n",
      "5.91728721721324\n",
      "(10,)\n",
      "7.824679722917151\n",
      "(10,)\n",
      "6.6519753630649205\n",
      "(10,)\n",
      "4.157070954373817\n",
      "(10,)\n",
      "5.679794293849045\n",
      "(10,)\n",
      "6.893755409960042\n",
      "(10,)\n",
      "6.470671426757928\n",
      "(10,)\n",
      "5.968959570600856\n",
      "(10,)\n",
      "6.977213161552009\n",
      "(10,)\n",
      "6.1898489680760544\n",
      "(10,)\n",
      "6.242969569191301\n",
      "(10,)\n",
      "4.031901725310504\n",
      "(10,)\n",
      "4.532572802687637\n",
      "(10,)\n",
      "4.151275896168794\n",
      "(10,)\n",
      "5.178179326862997\n",
      "(10,)\n",
      "6.259501359081475\n",
      "(10,)\n",
      "4.77573051533906\n",
      "(10,)\n",
      "5.097779546269558\n",
      "(10,)\n",
      "7.576517880724369\n",
      "(10,)\n",
      "6.833468506561872\n",
      "(10,)\n",
      "6.71829141344016\n",
      "(10,)\n",
      "7.264425909108706\n",
      "(10,)\n",
      "6.400719773410094\n",
      "(10,)\n",
      "5.006335794111692\n",
      "(10,)\n",
      "6.422128457249818\n",
      "(10,)\n",
      "6.508082927929156\n",
      "(10,)\n",
      "3.8455355447378725\n",
      "(10,)\n",
      "5.904276240442198\n",
      "(10,)\n",
      "5.604162271698991\n",
      "(10,)\n",
      "5.813714763720035\n",
      "(10,)\n",
      "6.165297579234337\n",
      "(10,)\n",
      "6.876183186699778\n",
      "(10,)\n",
      "7.032535812131898\n",
      "(10,)\n",
      "5.8273816059110075\n",
      "(10,)\n",
      "4.7506653420539555\n",
      "(10,)\n",
      "5.55508880916571\n",
      "(10,)\n",
      "5.131950148504688\n",
      "(10,)\n",
      "5.899132522062803\n",
      "(10,)\n",
      "4.0303426412857215\n",
      "(10,)\n",
      "6.26487377006526\n",
      "(10,)\n",
      "6.551117664775877\n",
      "(10,)\n",
      "6.709612384974173\n",
      "(10,)\n",
      "6.343756053283433\n",
      "(10,)\n",
      "4.2759676797728146\n",
      "(10,)\n",
      "7.690358753509044\n",
      "(10,)\n",
      "6.500613598138712\n",
      "(10,)\n",
      "6.5057867693774005\n",
      "(10,)\n",
      "4.77944506305745\n",
      "(10,)\n",
      "7.003881978187148\n",
      "(10,)\n",
      "5.39530984415298\n",
      "(10,)\n",
      "8.221852635610487\n",
      "(10,)\n",
      "4.595070211061043\n",
      "(10,)\n",
      "6.054198142872324\n",
      "(10,)\n",
      "5.979538383533098\n",
      "(10,)\n",
      "4.929059718884166\n",
      "(10,)\n",
      "4.333455355660418\n",
      "(10,)\n",
      "7.060044272016436\n",
      "(10,)\n",
      "7.405319342590362\n",
      "(10,)\n",
      "5.888165888015772\n",
      "(10,)\n",
      "7.44565914088223\n",
      "(10,)\n",
      "5.4092248005784205\n",
      "(10,)\n",
      "7.747468204926477\n",
      "(10,)\n",
      "6.902065208163011\n",
      "(10,)\n",
      "7.9393725003894495\n",
      "(10,)\n",
      "6.674684968456915\n",
      "(10,)\n",
      "4.568191436818716\n",
      "(10,)\n",
      "7.2603108506095015\n",
      "(10,)\n",
      "3.3175796446445514\n",
      "(10,)\n",
      "6.9812351080728385\n",
      "(10,)\n",
      "5.70538959982411\n",
      "(10,)\n",
      "5.806144992530135\n",
      "(10,)\n",
      "6.151942517743889\n",
      "(10,)\n",
      "4.589672304560051\n",
      "(10,)\n",
      "7.7880732048551415\n",
      "(10,)\n",
      "6.899865655201438\n",
      "(10,)\n",
      "6.96311176857974\n",
      "(10,)\n",
      "7.526802320631924\n",
      "(10,)\n",
      "7.5380798509969145\n",
      "(10,)\n",
      "4.475660797325224\n",
      "(10,)\n",
      "7.508670072993399\n",
      "(10,)\n",
      "6.473090727820141\n",
      "(10,)\n",
      "5.818397554453676\n",
      "(10,)\n",
      "6.715007760565617\n",
      "(10,)\n",
      "5.493598222775321\n",
      "(10,)\n",
      "8.125342248076162\n",
      "(10,)\n",
      "5.612472990295535\n",
      "(10,)\n",
      "4.628504328582664\n",
      "(10,)\n",
      "5.299488789187518\n",
      "(10,)\n",
      "6.424123703778267\n",
      "(10,)\n",
      "5.295713567800316\n",
      "(10,)\n",
      "6.65501228046611\n",
      "(10,)\n",
      "6.2837253098507295\n",
      "(10,)\n",
      "5.38182597492278\n",
      "(10,)\n",
      "8.038606718516235\n",
      "(10,)\n",
      "5.887675278234665\n",
      "(10,)\n",
      "6.017719159207877\n",
      "(10,)\n",
      "5.937042500262537\n",
      "(10,)\n",
      "3.3127481045638354\n",
      "(10,)\n",
      "7.5948745311433195\n",
      "(10,)\n",
      "5.716414726651485\n",
      "(10,)\n",
      "7.017349713901574\n",
      "(10,)\n",
      "4.986921919457903\n",
      "(10,)\n",
      "8.412380164411081\n",
      "(10,)\n",
      "6.252275829114897\n",
      "(10,)\n",
      "4.837936339491357\n",
      "(10,)\n",
      "6.772787987103274\n",
      "(10,)\n",
      "6.535725496422812\n",
      "(10,)\n",
      "5.935165386317577\n",
      "(10,)\n",
      "6.221518456365273\n",
      "(10,)\n",
      "6.672718721419833\n",
      "(10,)\n",
      "6.776055982962163\n",
      "(10,)\n",
      "5.992068208653665\n",
      "(10,)\n",
      "6.8068031350651435\n",
      "(10,)\n",
      "5.314514231013948\n",
      "(10,)\n",
      "5.386095443073068\n",
      "(10,)\n",
      "6.250766849866518\n",
      "(10,)\n",
      "5.3053438312893\n",
      "(10,)\n",
      "4.94494914534665\n",
      "(10,)\n",
      "6.663571572893841\n",
      "(10,)\n",
      "4.930694164208547\n",
      "(10,)\n",
      "3.86554731533074\n",
      "(10,)\n",
      "7.621817916980579\n",
      "(10,)\n",
      "7.220674860852849\n",
      "(10,)\n",
      "7.107991038923057\n",
      "(10,)\n",
      "5.287016345695978\n",
      "(10,)\n",
      "4.278733620729263\n",
      "(10,)\n",
      "6.299596912299403\n",
      "(10,)\n",
      "4.729762336083951\n",
      "(10,)\n",
      "5.729253461203276\n",
      "(10,)\n",
      "7.31362498954804\n",
      "(10,)\n",
      "5.44217347601395\n",
      "(10,)\n",
      "7.344405880130405\n",
      "(10,)\n",
      "7.205231627117185\n",
      "(10,)\n",
      "7.813941469274633\n",
      "(10,)\n",
      "7.8519369444974\n",
      "(10,)\n",
      "6.6094003404645125\n",
      "(10,)\n",
      "4.344173615192512\n",
      "(10,)\n",
      "5.460079630857912\n",
      "(10,)\n",
      "4.381412981906919\n",
      "(10,)\n",
      "6.10738915685192\n",
      "(10,)\n",
      "5.5558607886635105\n",
      "(10,)\n",
      "7.910022328816175\n",
      "(10,)\n",
      "7.142955696789925\n",
      "(10,)\n",
      "7.428556788730669\n",
      "(10,)\n",
      "6.672901643087284\n",
      "(10,)\n",
      "5.019111782619942\n",
      "(10,)\n",
      "3.8085318114854037\n",
      "(10,)\n",
      "7.1478550505563545\n",
      "(10,)\n",
      "6.196704487364402\n",
      "(10,)\n",
      "5.24034130939436\n",
      "(10,)\n",
      "5.395242492002445\n",
      "(10,)\n",
      "7.420817308028636\n",
      "(10,)\n",
      "5.106935022402654\n",
      "(10,)\n",
      "5.956619605483651\n",
      "(10,)\n",
      "6.714463752591062\n",
      "(10,)\n",
      "6.23240751532928\n",
      "(10,)\n",
      "4.9396669623960765\n",
      "(10,)\n",
      "4.966825847217973\n",
      "(10,)\n",
      "4.2603176078192\n",
      "(10,)\n",
      "7.189692088105988\n",
      "(10,)\n",
      "5.294759435639506\n",
      "(10,)\n",
      "4.944698651127796\n",
      "(10,)\n",
      "7.899292671644931\n",
      "(10,)\n",
      "5.548547692654624\n",
      "(10,)\n",
      "5.632411607699811\n",
      "(10,)\n",
      "7.3385712024871\n",
      "(10,)\n",
      "7.326540143150389\n",
      "(10,)\n",
      "6.552036089443334\n",
      "(10,)\n",
      "6.322173014809681\n",
      "(10,)\n",
      "5.565001934565506\n",
      "(10,)\n",
      "5.920668473142102\n",
      "(10,)\n",
      "4.123821193437038\n",
      "(10,)\n",
      "5.642079849818387\n",
      "(10,)\n",
      "5.820338782227857\n",
      "(10,)\n",
      "5.962996668401891\n",
      "(10,)\n",
      "5.761196586492977\n",
      "(10,)\n",
      "5.947133195274109\n",
      "(10,)\n",
      "7.25022423835843\n",
      "(10,)\n",
      "6.5317883323810335\n",
      "(10,)\n",
      "6.928351525382079\n",
      "(10,)\n",
      "7.151768702091716\n",
      "(10,)\n",
      "5.244914182867891\n",
      "(10,)\n",
      "7.089033208517047\n",
      "(10,)\n",
      "7.042582708972428\n",
      "(10,)\n",
      "5.966496988778702\n",
      "(10,)\n",
      "4.205712807968039\n",
      "(10,)\n",
      "7.141614125603874\n",
      "(10,)\n",
      "6.0797973327694415\n",
      "(10,)\n",
      "6.644955361115845\n",
      "(10,)\n",
      "5.537689872762472\n",
      "(10,)\n",
      "7.626894675973576\n",
      "(10,)\n",
      "5.866841851293183\n",
      "(10,)\n",
      "3.702684158790938\n",
      "(10,)\n",
      "6.340007309767776\n",
      "(10,)\n",
      "5.1277699607472105\n",
      "(10,)\n",
      "6.14568578591161\n",
      "(10,)\n",
      "7.2358819986360094\n",
      "(10,)\n",
      "7.409226891481782\n",
      "(10,)\n",
      "7.446343729241718\n",
      "(10,)\n",
      "6.724923859652895\n",
      "(10,)\n",
      "7.441871474076331\n",
      "(10,)\n",
      "5.478867114199484\n",
      "(10,)\n",
      "6.606010353586782\n",
      "(10,)\n",
      "3.435096494519159\n",
      "(10,)\n",
      "3.7025717604429875\n",
      "(10,)\n",
      "6.412999633289457\n",
      "(10,)\n",
      "6.130574356388563\n",
      "(10,)\n",
      "6.9561763507374\n",
      "(10,)\n",
      "3.845851050744754\n",
      "(10,)\n",
      "6.312516353437878\n",
      "(10,)\n",
      "4.5103808366126925\n",
      "(10,)\n",
      "5.975178250614366\n",
      "(10,)\n",
      "4.888823119651512\n",
      "(10,)\n",
      "7.295598948674054\n",
      "(10,)\n",
      "6.582858973201501\n",
      "(10,)\n",
      "4.811090823686973\n",
      "(10,)\n",
      "8.066976575650893\n",
      "(10,)\n",
      "4.024250805358414\n",
      "(10,)\n",
      "5.768310942228452\n",
      "(10,)\n",
      "7.178091724311738\n",
      "(10,)\n",
      "8.723044844509978\n",
      "(10,)\n",
      "6.557720616150761\n",
      "(10,)\n",
      "7.090100592406513\n",
      "(10,)\n",
      "5.88163422866868\n",
      "(10,)\n",
      "3.8142485264877357\n",
      "(10,)\n",
      "4.415188725859913\n",
      "(10,)\n",
      "4.856047449741121\n",
      "(10,)\n",
      "6.573521333231656\n",
      "(10,)\n",
      "6.936302387393097\n",
      "(10,)\n",
      "6.844401942142337\n",
      "(10,)\n",
      "5.9835377073385185\n",
      "(10,)\n",
      "7.652639504171689\n",
      "(10,)\n",
      "5.051563646525463\n",
      "(10,)\n",
      "5.589296349913615\n",
      "(10,)\n",
      "7.721236270613531\n",
      "(10,)\n",
      "8.277475900595242\n",
      "(10,)\n",
      "6.275624964802562\n",
      "(10,)\n",
      "5.551101344028196\n",
      "(10,)\n",
      "5.326677977647853\n",
      "(10,)\n",
      "4.643650307314561\n",
      "(10,)\n",
      "6.784706902137736\n",
      "(10,)\n",
      "8.248452897848928\n",
      "(10,)\n",
      "5.905729603297287\n",
      "(10,)\n",
      "4.434604484121088\n",
      "(10,)\n",
      "5.5051142223702225\n",
      "(10,)\n",
      "5.366381735788467\n",
      "(10,)\n",
      "6.466276463354948\n",
      "(10,)\n",
      "7.457181363444924\n",
      "(10,)\n",
      "6.709187748623302\n",
      "(10,)\n",
      "5.867079929913203\n",
      "(10,)\n",
      "5.384848808452688\n",
      "(10,)\n",
      "6.901559713351555\n",
      "(10,)\n",
      "6.426692077984358\n",
      "(10,)\n",
      "3.8928286607561517\n",
      "(10,)\n",
      "5.723050993587478\n",
      "(10,)\n",
      "8.078996490159602\n",
      "(10,)\n",
      "6.450659739981292\n",
      "(10,)\n",
      "7.776021241346395\n",
      "(10,)\n",
      "6.332930249491413\n",
      "(10,)\n",
      "6.08987078655768\n",
      "(10,)\n",
      "6.6962327191221345\n",
      "(10,)\n",
      "7.686333718122455\n",
      "(10,)\n",
      "6.763339674220799\n",
      "(10,)\n",
      "5.9369652267025765\n",
      "(10,)\n",
      "7.380384147095913\n",
      "(10,)\n",
      "7.0190766057494995\n",
      "(10,)\n",
      "5.669194195616743\n",
      "(10,)\n",
      "6.1998256345898906\n",
      "(10,)\n",
      "5.986394879532815\n",
      "(10,)\n",
      "6.767135936148446\n",
      "(10,)\n",
      "5.414106599987978\n",
      "(10,)\n",
      "5.889448523468994\n",
      "(10,)\n",
      "6.152339906693715\n",
      "(10,)\n",
      "6.808835557604614\n",
      "(10,)\n",
      "6.049108099871477\n",
      "(10,)\n",
      "7.768942791822287\n",
      "(10,)\n",
      "6.859345841741938\n",
      "(10,)\n",
      "7.680928300409064\n",
      "(10,)\n",
      "7.600417376730908\n",
      "(10,)\n",
      "8.331444700287074\n",
      "(10,)\n",
      "7.432654621779302\n",
      "(10,)\n",
      "4.174190995561354\n",
      "(10,)\n",
      "6.185912841535151\n",
      "(10,)\n",
      "4.9877130738225794\n",
      "(10,)\n",
      "7.512409713131936\n",
      "(10,)\n",
      "6.786572548757137\n",
      "(10,)\n",
      "5.084160932692256\n",
      "(10,)\n",
      "6.761209233229694\n",
      "(10,)\n",
      "6.568564713924005\n",
      "(10,)\n",
      "4.5172454426226025\n",
      "(10,)\n",
      "7.311443232201938\n",
      "(10,)\n",
      "6.106369035408945\n",
      "(10,)\n",
      "5.316345239776264\n",
      "(10,)\n",
      "6.869475909119228\n",
      "(10,)\n",
      "6.865651054215991\n",
      "(10,)\n",
      "6.145726796735409\n",
      "(10,)\n",
      "7.73863808085866\n",
      "(10,)\n",
      "4.605770197302424\n",
      "(10,)\n",
      "6.002972915446552\n",
      "(10,)\n",
      "5.35851705432652\n",
      "(10,)\n",
      "5.4773297269926555\n",
      "(10,)\n",
      "6.021840729795238\n",
      "(10,)\n",
      "6.0562258635839665\n",
      "(10,)\n",
      "3.6041004584460277\n",
      "(10,)\n",
      "5.119255185714129\n",
      "(10,)\n",
      "6.8892689729345395\n",
      "(10,)\n",
      "4.712478067782635\n",
      "(10,)\n",
      "7.71331383059724\n",
      "(10,)\n",
      "5.657426334355964\n",
      "(10,)\n",
      "6.569066764387452\n",
      "(10,)\n",
      "4.367185490081581\n",
      "(10,)\n",
      "7.301943541798204\n",
      "(10,)\n",
      "3.9149704637837575\n",
      "(10,)\n",
      "6.305131300661113\n",
      "(10,)\n",
      "7.615529462997456\n",
      "(10,)\n",
      "3.14090706322553\n",
      "(10,)\n",
      "5.499097747384652\n",
      "(10,)\n",
      "6.9963115377122165\n",
      "(10,)\n",
      "4.327018210346814\n",
      "(10,)\n",
      "5.911813259566192\n",
      "(10,)\n",
      "7.226800203132229\n",
      "(10,)\n",
      "6.860953283253934\n",
      "(10,)\n",
      "5.032615592368231\n",
      "(10,)\n",
      "7.572423141991325\n",
      "(10,)\n",
      "7.80131326503007\n",
      "(10,)\n",
      "6.947970821790882\n",
      "(10,)\n",
      "6.197909267854142\n",
      "(10,)\n",
      "4.283177013886599\n",
      "(10,)\n",
      "6.827860529167807\n",
      "(10,)\n",
      "3.532555530084017\n",
      "(10,)\n",
      "5.405545887309267\n",
      "(10,)\n",
      "3.2845378619656116\n",
      "(10,)\n",
      "7.62862676084049\n",
      "loss: 1178.892300\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ 10 classes, softmax function guarantees that its sum is 1 => 1/10 is expected average.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 707.238305 analytic: 1.411653, relative error: 9.960159e-01\n",
      "numerical: -388.465836 analytic: -0.775381, relative error: 9.960159e-01\n",
      "numerical: 16.675259 analytic: 0.033284, relative error: 9.960159e-01\n",
      "numerical: -7.481434 analytic: -0.014933, relative error: 9.960159e-01\n",
      "numerical: -2535.620058 analytic: -5.061118, relative error: 9.960159e-01\n",
      "numerical: 288.673217 analytic: 0.576194, relative error: 9.960159e-01\n",
      "numerical: 1449.859804 analytic: 2.893932, relative error: 9.960159e-01\n",
      "numerical: -621.772804 analytic: -1.241064, relative error: 9.960159e-01\n",
      "numerical: 2193.024216 analytic: 4.377294, relative error: 9.960159e-01\n",
      "numerical: -559.992403 analytic: -1.117749, relative error: 9.960159e-01\n",
      "\n",
      "\n",
      "numerical: 660.373818 analytic: 1.315801, relative error: 9.960229e-01\n",
      "numerical: 1663.134217 analytic: 3.318806, relative error: 9.960169e-01\n",
      "numerical: -1472.643804 analytic: -2.944495, relative error: 9.960091e-01\n",
      "numerical: -1124.245733 analytic: -2.238997, relative error: 9.960248e-01\n",
      "numerical: -312.700994 analytic: -0.626709, relative error: 9.959997e-01\n",
      "numerical: 262.177293 analytic: 0.520113, relative error: 9.960402e-01\n",
      "numerical: 356.860935 analytic: 0.713273, relative error: 9.960105e-01\n",
      "numerical: -380.715300 analytic: -0.751059, relative error: 9.960623e-01\n",
      "numerical: -203.228193 analytic: -0.397572, relative error: 9.960951e-01\n",
      "numerical: -985.253274 analytic: -1.962998, relative error: 9.960232e-01\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "print('\\n')\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuning",
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "\n",
    "# Provided as a reference. You may or may not want to change these hyperparameters\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "pass\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test"
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ False\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$ It IS possible with softmax loss when $$f_{y_i}=log\\sum \\limits _{j}e^{f_j}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# IMPORTANT\n",
    "\n",
    "This is the end of this question. Please do the following:\n",
    "\n",
    "1. Click `File -> Save` to make sure the latest checkpoint of this notebook is saved to your Drive.\n",
    "2. Execute the cell below to download the modified `.py` files back to your drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "FOLDER_TO_SAVE = os.path.join('drive/My Drive/', FOLDERNAME)\n",
    "FILES_TO_SAVE = ['cs231n/classifiers/softmax.py']\n",
    "\n",
    "for files in FILES_TO_SAVE:\n",
    "  with open(os.path.join(FOLDER_TO_SAVE, '/'.join(files.split('/')[1:])), 'w') as f:\n",
    "    f.write(''.join(open(files).readlines()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
